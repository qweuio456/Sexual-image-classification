{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "#from scipy.speical import expit\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "#from keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "WIDTH_NORM = 224\n",
    "HEIGHT_NORM = 224\n",
    "GRID_NUM = 11\n",
    "X_SPAN = WIDTH_NORM / GRID_NUM\n",
    "Y_SPAN = HEIGHT_NORM / GRID_NUM\n",
    "X_NORM = WIDTH_NORM / GRID_NUM\n",
    "Y_NORM = HEIGHT_NORM / GRID_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    x = conv2D(filters1, (1, 1), name = conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis = bn_axis, name = bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = conv2D(filters2, kernel_size, padding = 'same', name = conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis = bn_axis, name = bn_name_base + \"2b\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = conv2D(filters3, (1, 1), name = conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis = bn_axis, name = bn_name_base + '2c')(x)\n",
    "    \n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides = (2, 2)):\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    x = Conv2D(filters1, (1, 1), strides = strides, name = conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis = bn_axis, name = bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters2, kernel_size, padding = 'same', name = conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis = bn_axis, name = bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters3, (1, 1), name = conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis = bn_axis, name = bn_name_base + '2c')(x)\n",
    "    \n",
    "    shortcut = Conv2D(filters3, (1, 1), strides = strides, name = conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis = bn_axis, name = bn_name_base + '1')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(include_top = False, load_weight = True, weights = 'imagenet', input_tensor = None, input_shape = None, pooling = None, classes = 1000):\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('pass')\n",
    "    \n",
    "    input_shape = _obtain_input_shape(input_shape, default_size = 224, min_size = 197, data_format = K.image_data_format(), require_flatten = include_top)\n",
    "    \n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape = input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor = input_tensor, shape = input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1')(x)\n",
    "    x = BatchNormalization(axis = bn_axis, name = 'bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "    \n",
    "    x = conv_block(x, 3, [64, 64, 256], stage = 2, block = 'a', strides = (1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage = 2, block = 'b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage = 2, block = 'c')\n",
    "    \n",
    "    x = conv_block(x, 3, [128, 128, 512], stage = 3, block = 'a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage = 3, block = 'b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage = 3, block = 'c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage = 3, block = 'd')\n",
    "    \n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage = 4, block = 'a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage = 4, block = 'b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage = 4, block = 'c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage = 4, block = 'd')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage = 4, block = 'e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage = 4, block = 'f')\n",
    "    \n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage = 5, block = 'a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage = 5, block = 'b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage = 5, block = 'c')\n",
    "    \n",
    "    x = AveragePooling2D((7, 7), name = 'avg_pool')(x)\n",
    "    \n",
    "    if include_top:\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, activation = 'softmax', name = 'fc1000')(x)\n",
    "    else:\n",
    "        x = Flatten(name = 'yolo_clf_0')(x)\n",
    "        x = Dense(2048, activation = 'relu', name = 'yolo_clf_1')(x)\n",
    "        x = Dropout(0.5, name = 'yolo_clf_2')(x)\n",
    "        \n",
    "        x = Dense(11 * 11 * (3 + 2 * 5), activation = 'linear', name = 'yolo_clf_3')(x)\n",
    "        \n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    model = Model(inputs, x, name = 'resnet50_yolo')\n",
    "    \n",
    "    if load_weight:\n",
    "        if weights == 'imagenet':\n",
    "            if include_top:\n",
    "                weights_path = 'models/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "            else:\n",
    "                weights_path = 'models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        else:\n",
    "            weights_path = weights\n",
    "            \n",
    "        model.load_weights(weights_path, by_name = True)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_body(t_true, t_pred, i, ta):\n",
    "    c_true = t_true[i]\n",
    "    c_pred = t_pred[i]\n",
    "    \n",
    "    # 텐서 좌표에 sigmoid 도포하여 예상대로 0~1 사이 스케일링\n",
    "    c_pred = tf.concat((c_pred[:605], tf.sigmoid(c_pred[-968:])), axis = 0)\n",
    "    \n",
    "    xywh_true = tf.reshape(c_true[-968:], (11, 11, 2, 4))\n",
    "    xywh_pred = tf.reshape(c_pred[-968:], (11, 11, 2, 4))\n",
    "    \n",
    "    # 정규화된 값을 실제 값으로 변환(그리드 셀 크기에 비례)\n",
    "    x_true = xywh_true[:, :, :, 0] * X_NORM\n",
    "    x_pred = xywh_pred[:, :, :, 0] * X_NORM\n",
    "    \n",
    "    y_true = xywh_true[:, :, :, 1] * Y_NORM\n",
    "    y_pred = xywh_pred[:, :, :, 1] * Y_NORM\n",
    "    \n",
    "    w_true = xywh_true[:, :, :, 2] * WIDTH_NORM\n",
    "    w_pred = xywh_pred[:, :, :, 2] * WIDTH_NORM\n",
    "    \n",
    "    h_true = xywh_true[:, :, :, 3] * HEIGHT_NORM\n",
    "    h_pred = xywh_pred[:, :, :, 3] * HEIGHT_NORM\n",
    "    \n",
    "    x_dist = tf.abs(tf.subtract(x_true, x_pred)) # tensorflow 뺄셈\n",
    "    y_dist = tf.abs(tf.subtract(y_true, y_pred))\n",
    "    \n",
    "    wwd = tf.nn.relu(w_true / 2 + w_pred / 2 - x_dist)\n",
    "    hhd = tf.nn.relu(h_true / 2 + h_pred / 2 - y_dist)\n",
    "    \n",
    "    area_true = tf.multiply(w_true, h_true) # tensorflow 곱셈\n",
    "    area_pred = tf.multiply(w_pred, h_pred)\n",
    "    area_intersection = tf.multiply(wwd, hhd)\n",
    "    \n",
    "    # iou : 보통 두 가지 물체의 위치(Bounding Box)가 얼마나 일치하는지를 수학적으로 나타내는 지표\n",
    "    iou = area_intersection / (area_true + area_pred - area_intersection + 1e-4)\n",
    "    confidence_true = tf.reshape(iou, (-1, ))\n",
    "    \n",
    "    grid_true = tf.reshape(c_true[:363], (11, 11, 3))\n",
    "    grid_true_sum = tf.reduce_sum(grid_true, axis = 2)\n",
    "    grid_true_exp = tf.stack((grid_true_sum, grid_true_sum), axis = 2)\n",
    "    grid_true_exp3 = tf.stack((grid_true_sum, grid_true_sum, grid_true_sum), axis = 2)\n",
    "    grid_true_exp4 = tf.stack((grid_true_sum, grid_true_sum, grid_true_sum, grid_true_sum), axis = 2)\n",
    "    \n",
    "    coord_mask = tf.reshape(grid_true_exp4, (-1, ))\n",
    "    confidence_mask = tf.reshape(grid_true_exp, (-1, ))\n",
    "    confidence_true = confidence_true * confidence_mask\n",
    "    \n",
    "    # 계산된 신뢰도 값에 기반하고 비객체 그리드가 억제된 수정된 지상 실측 텐서\n",
    "    c_true_new = tf.concat([c_true[:363], confidence_true, c_true[-968:]], axis = 0)\n",
    "    \n",
    "    # 손실 계산을 위해 그리드 셀에 '책임 있는' 경계 상자에 대한 마스크 생성\n",
    "    confidence_true_matrix = tf.reshape(confidence_true, (11, 11, 2))\n",
    "    confidence_true_argmax = tf.argmax(confidence_true_matrix, axis = 2)\n",
    "    confidence_true_argmax = tf.cast(confidence_true_argmax, tf.int32)\n",
    "    ind_i, ind_j = tf.meshgrid(tf.range(11), tf.range(11), indexing = 'ij') # 좌표벡터를 사용해 행렬을 만듬\n",
    "    ind_argmax = tf.stack((ind_i, ind_j, confidence_true_argmax), axis = 2)\n",
    "    ind_argmax = tf.reshape(ind_argmax, (121, 3))\n",
    "    \n",
    "    responsible_mask_2 = tf.scatter_nd(ind_argmax, tf.ones((121)), [11, 11, 2])\n",
    "    responsible_mask_2 = tf.reshape(responsible_mask_2, (-1, ))\n",
    "    responsible_mask_2 = responsible_mask_2 * confidence_mask\n",
    "    \n",
    "    responsible_mask_4 = tf.scatter_nd(ind_argmax, tf.ones((121, 2)), [11, 11, 2, 2])\n",
    "    responsible_mask_4 = tf.reshape(responsible_mask_4, (-1, ))\n",
    "    responsible_mask_4 = responsible_mask_4 * coord_mask\n",
    "    \n",
    "    # 나머지 경계 상자에 대한 mask\n",
    "    inv_responsible_mask_2 = tf.cast(tf.logical_not(tf.cast(responsible_mask_2, tf.bool)), tf.float32)\n",
    "    inv_responsible_mask_4 = tf.cast(tf.logical_not(tf.cast(responsible_mask_4, tf.bool)), tf.float32)\n",
    "    \n",
    "    # lambda 값 정의\n",
    "    lambda_coord = 5.0\n",
    "    lambda_noobj = 0.5\n",
    "    \n",
    "    # loss dimensions\n",
    "    dims_true = tf.reshape(c_true_new[-968:], (11, 11, 2, 4))\n",
    "    dims_pred = tf.reshape(c_pred[-968:], (11, 11, 2, 4))\n",
    "    \n",
    "    xy_true = tf.reshape(dims_true[:, :, :, :2], (-1, ))\n",
    "    xy_pred = tf.reshape(dims_pred[:, :, :, :2], (-1, ))\n",
    "    \n",
    "    wh_true = tf.reshape(dims_true[:, :, :, 2:], (-1, ))\n",
    "    wh_pred = tf.reshape(dims_pred[:, :, :, 2:], (-1, ))\n",
    "    \n",
    "    # xy difference loss\n",
    "    xy_loss = (xy_true - xy_pred) * responsible_mask_4\n",
    "    xy_loss = tf.square(xy_loss)\n",
    "    xy_loss = lambda_coord * tf.reduce_sum(xy_loss)\n",
    "    \n",
    "    # wh 제곱근 difference loss\n",
    "    wh_loss = (tf.sqrt(wh_true) - tf.sqrt(tf.abs(wh_pred))) * responsible_mask_4\n",
    "    wh_loss = tf.square(wh_loss)\n",
    "    wh_loss = lambda_coord * tf.reduce_sum(xy_loss)\n",
    "    \n",
    "    # conf losses\n",
    "    conf_true = c_true_new[363:605]\n",
    "    conf_pred = c_pred[363:605]\n",
    "    \n",
    "    conf_loss_obj = (conf_true - conf_pred) * responsible_mask_2\n",
    "    conf_loss_obj = tf.square(conf_loss_obj)\n",
    "    conf_loss_obj = tf.reduce_sum(conf_loss_obj)\n",
    "    \n",
    "    conf_loss_noobj = (conf_true - conf_pred) * inv_responsible_mask_2\n",
    "    conf_loss_noobj = tf.square(conf_loss_noobj)\n",
    "    conf_loss_noobj = lambda_noobj * tf.reduce_sum(conf_loss_noobj)\n",
    "    \n",
    "    # class prediction loss\n",
    "    class_true = tf.reshape(c_true_new[:363], (11, 11, 3))\n",
    "    class_pred = tf.reshape(c_pred[:363], (11, 11, 3))\n",
    "    class_pred_softmax = class_pred # tf.nn.softmax(class_pred)\n",
    "    \n",
    "    classification_loss = class_true - class_pred_sotfmax\n",
    "    classification_loss = classification_loss * grid_true_exp3\n",
    "    classification_loss = tf.square(classification_loss)\n",
    "    classification_loss = tf.reduce_sum(classification_loss)\n",
    "    \n",
    "    total_loss = xy_loss + wh_loss + conf_loss_obj + conf_loss_noobj + classification_loss\n",
    "    \n",
    "    ta = ta.write(i, total_loss)\n",
    "    i = i + 1\n",
    "    \n",
    "    return t_true, t_pred, i, ta\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    # 기본값이 없기 때문에 keras에서 손실 함수를 구함\n",
    "    c = lambda t, p, i, ta : tf.less(i, tf.shape(t)[0])\n",
    "    ta = tf.TensorArray(tf.float32, size = 1, dynamic_size = True)\n",
    "    \n",
    "    t, p, i, ta = tf.while_loop(c, loop_body, [y_true, y_pred, 0, ta])\n",
    "    \n",
    "    loss_tensor = ta.stack()\n",
    "    loss_mean = tf.reduce_mean(loss_tensor)\n",
    "    \n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_translate(bboxes, tr_x, tr_y):\n",
    "    new_list = []\n",
    "    for box in bboxes:\n",
    "        coords = np.array(box[0])\n",
    "        coords[:, 0] = coords[:, 0] + tr_x\n",
    "        coords[:, 1] = coords[:, 1] + tr_y\n",
    "        coords = coords.astype(np.int64)\n",
    "        out_of_bound_indices = np.average(coords, axis = 0) >= 224\n",
    "        \n",
    "        if out_of_bound_indices.any():\n",
    "            continue\n",
    "        coords = coords.tolist()\n",
    "        new_list.append((coords, box[1]))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_scale(bboxes, sc):\n",
    "    new_list = []\n",
    "    for box in bboxes:\n",
    "        coords = np.array(box[0])\n",
    "        coords = coords * sc\n",
    "        coords = coords.astype(np.int64)\n",
    "        out_of_bound_indices = np.average(coords, axis = 0) >= 224\n",
    "        if out_of_bound_indices.any():\n",
    "            continue\n",
    "        coords = coordsd.tolist()\n",
    "        new_list.append((coords, box[1]))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(label_keys, label_frames, batch_size = 64):\n",
    "    num_samples = len(label_keys)\n",
    "    indx = label_keys\n",
    "    \n",
    "    while 1:\n",
    "        shuffle(indx)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = indx[offset:offset + batch_size]\n",
    "            \n",
    "            images = []\n",
    "            gt = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                im, frame = augument_data(batch_sample, label_frames[batch_sample])\n",
    "                im = im.astype(np.float32)\n",
    "                im -= 128\n",
    "                images.append(im)\n",
    "                frame_tensor = label_to_tensor(frame)\n",
    "                gt.append(frame_tensor)\n",
    "                \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(gt)\n",
    "            yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'label_frames.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-34a9018bb96f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label_frames.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mlabel_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'label_frames.p'"
     ]
    }
   ],
   "source": [
    "#with open('label_frames.p', 'rb') as f:\n",
    " #   label_frames = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-db8b0a73d158>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_frames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlbl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlbl_validn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbl_validn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_frames' is not defined"
     ]
    }
   ],
   "source": [
    "#label_keys = list(label_frames.keys())\n",
    "#lbl_train, lbl_validn = train_test_split(label_keys, test_size = 0.2)\n",
    "\n",
    "#train_generator = generator(lbl_train, label_frames)\n",
    "#validation_generator = generator(lbl_validn, label_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = Adam(lr = 0.001)\n",
    "#model.compile(optimizer = optimizer, loss = custom_loss)\n",
    "#model.fit_generator(train_generator, validation_data = validation_generator, \n",
    "                   steps_per_epoch = )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
